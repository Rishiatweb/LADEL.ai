{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP3kUFYuqo9IpazjMZ5QMW2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishiatweb/LADEL.ai/blob/main/pipeline/LADEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVsVW4EN9Fsi",
        "outputId": "a310dabf-c442-4002-9ccd-25a494238e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required libraries\n",
            "✅ Dependencies installed successfully.\n"
          ]
        }
      ],
      "source": [
        "print(\"Installing required libraries\")\n",
        "!pip install -q sentence-transformers scikit-learn pandas numpy pyyaml transformers accelerate torch\n",
        "print(\"✅ Dependencies installed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"ladel\", exist_ok=True)\n",
        "print(\"Project directory 'ladel/' created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKW4hIkrFcNq",
        "outputId": "4b245cdb-ab7e-43f3-ba3e-609d56f192af"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project directory 'ladel/' created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to create configuration file\n",
        "%%writefile config.yaml\n",
        "##dont touch this if you want to use this pipeline,only to experiment\n",
        "\n",
        "# --- Embedding Settings ---\n",
        "embedding_model: 'all-MiniLM-L6-v2' # A fast, good-quality model\n",
        "\n",
        "# --- Anomaly Detection Settings ---\n",
        "# You can choose from: 'isolation_forest', 'lof', 'one_class_svm'\n",
        "detection_model: 'isolation_forest'\n",
        "\n",
        "isolation_forest:\n",
        "  n_estimators: 100\n",
        "  contamination: 0.1 # Expected proportion of anomalies. 'auto' is also an option.\n",
        "  random_state: 42\n",
        "\n",
        "local_outlier_factor:\n",
        "  n_neighbors: 20\n",
        "  contamination: 0.1\n",
        "  novelty: True # Must be True to use predict() on new data\n",
        "\n",
        "one_class_svm:\n",
        "  kernel: 'rbf'\n",
        "  nu: 0.1 # An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Similar to contamination.\n",
        "\n",
        "# --- Explanation Settings ---\n",
        "explanation_enabled: true\n",
        "llm_model: 'google/flan-t5-base'\n",
        "max_new_tokens: 256\n",
        "num_normal_samples_for_context: 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LllRVAKG9elw",
        "outputId": "5d55c810-506b-4b02-ac69-b5dbab4a45e8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ladel/data_loader.py\n",
        "import os\n",
        "def load_log_file(filepath: str) -> list[str]:\n",
        "  \"\"\"loads a log file, decodes it, and processes it.\"\"\"\n",
        "  if not os.path.exists(filepath):\n",
        "    print(f\"Error: File not found at {filepath}\")\n",
        "    return []\n",
        "\n",
        "  print(f\"Reading log file from: {filepath}\")\n",
        "  try:\n",
        "      with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "          log_content = f.read()\n",
        "\n",
        "      log_lines = log_content.splitlines()\n",
        "      processed_lines = [line.strip() for line in log_lines if line.strip()]\n",
        "\n",
        "      print(f\"Read {len(processed_lines)} non-empty log lines.\")\n",
        "      return processed_lines\n",
        "  except Exception as e:\n",
        "      print(f\"Error reading or processing file: {e}\")\n",
        "      return []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn8m7RpuIA4S",
        "outputId": "ee23b4a6-c8f6-41f9-e7df-9d574319bba2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ladel/data_loader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Create the Embedding Module\n",
        "%%writefile ladel/embedding.py\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "class LogEmbedder:\n",
        "    def __init__(self, model_name: str):\n",
        "        \"\"\"Initializes the sentence transformer model.\"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.embedder = None\n",
        "        print(f\"\\nLoading sentence transformer model: '{self.model_name}'...\")\n",
        "        try:\n",
        "            self.embedder = SentenceTransformer(self.model_name)\n",
        "            print(\"Model loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading embedding model: {e}\")\n",
        "\n",
        "    def encode(self, log_lines: list[str]) -> np.ndarray | None:\n",
        "        \"\"\"Generates embeddings for a list of log lines.\"\"\"\n",
        "        if not self.embedder or not log_lines:\n",
        "            print(\"Embedder not loaded or no log lines to process. Skipping encoding.\")\n",
        "            return None\n",
        "\n",
        "        print(\"Generating embeddings for log lines...\")\n",
        "        try:\n",
        "            embeddings = self.embedder.encode(log_lines, show_progress_bar=True)\n",
        "            print(f\"Generated {embeddings.shape[0]} embeddings with dimension {embeddings.shape[1]}.\")\n",
        "            return embeddings\n",
        "        except Exception as e:\n",
        "            print(f\"Error during embedding generation: {e}\")\n",
        "            return None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrBf_vsAJRLk",
        "outputId": "0bdb7d94-76e0-42e6-ded2-ec187d8e7279"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ladel/embedding.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Create the Anomaly Detection Module\n",
        "%%writefile ladel/detection.py\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "def get_anomaly_detector(model_name: str, params: dict):\n",
        "    \"\"\"Factory function to get an anomaly detection model.\"\"\"\n",
        "    print(f\"\\nInitializing detection model: {model_name}\")\n",
        "    if model_name == 'isolation_forest':\n",
        "        params['contamination'] = float(params['contamination']) if str(params['contamination']) != 'auto' else 'auto'\n",
        "        params['n_estimators'] = int(params['n_estimators'])\n",
        "        return IsolationForest(**params)\n",
        "    elif model_name == 'lof':\n",
        "        params['n_neighbors'] = int(params['n_neighbors'])\n",
        "        params['contamination'] = float(params['contamination'])\n",
        "        return LocalOutlierFactor(**params)\n",
        "    elif model_name == 'one_class_svm':\n",
        "        params['nu'] = float(params['nu'])\n",
        "        return OneClassSVM(**params)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown detection model: {model_name}\")\n",
        "\n",
        "def train_and_predict(model, embeddings: np.ndarray) -> tuple[np.ndarray | None, np.ndarray | None]:\n",
        "    \"\"\"Trains the model and returns predictions and indices of anomalies.\"\"\"\n",
        "    if embeddings is None or embeddings.shape[0] == 0:\n",
        "        print(\"No embeddings available. Skipping anomaly detection.\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"Training {model.__class__.__name__} for anomaly detection...\")\n",
        "    try:\n",
        "        # LocalOutlierFactor uses fit_predict for training data\n",
        "        if isinstance(model, LocalOutlierFactor):\n",
        "            predictions = model.fit_predict(embeddings)\n",
        "        else:\n",
        "            model.fit(embeddings)\n",
        "            predictions = model.predict(embeddings)\n",
        "\n",
        "        anomalous_indices = np.where(predictions == -1)[0]\n",
        "        print(f\"Model identified {len(anomalous_indices)} potential anomalies.\")\n",
        "        return predictions, anomalous_indices\n",
        "    except Exception as e:\n",
        "        print(f\"Error during model training or prediction: {e}\")\n",
        "        return None, None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhnnzu37JTib",
        "outputId": "d3da37b0-cc61-474b-f1e6-81a6e7468830"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ladel/detection.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Create the Explanation Module\n",
        "%%writefile ladel/explanation.py\n",
        "from transformers import pipeline, logging\n",
        "import torch\n",
        "import textwrap\n",
        "\n",
        "# Suppress verbose logging from transformers to keep output clean\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "class LogExplainer:\n",
        "    def __init__(self, model_name: str):\n",
        "        \"\"\"Initializes the text-generation pipeline for explanations.\"\"\"\n",
        "        self.generator = None\n",
        "        print(\"\\nSetting up Hugging Face text-generation pipeline...\")\n",
        "        device = 0 if torch.cuda.is_available() else -1\n",
        "        if device == 0:\n",
        "            print(f\"GPU found. Loading model '{model_name}' to GPU.\")\n",
        "        else:\n",
        "            print(f\"GPU not available. Loading model '{model_name}' to CPU (will be slow).\")\n",
        "\n",
        "        try:\n",
        "            self.generator = pipeline('text2text-generation', model=model_name, device=device)\n",
        "            print(\"LLM explainer loaded successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading LLM explainer model: {e}\")\n",
        "\n",
        "    def generate_explanation(self, anomaly_log: str, normal_logs_sample: list[str], max_new_tokens: int = 128) -> str:\n",
        "        \"\"\"Generates a concise explanation for a single anomaly.\"\"\"\n",
        "        if not self.generator:\n",
        "            return \"LLM explainer not available.\"\n",
        "\n",
        "        normal_logs_text = \"\\n\".join(normal_logs_sample)\n",
        "        prompt_template = \"\"\"\n",
        "### INSTRUCTIONS ###\n",
        "You are an expert log analysis assistant. Your task is to explain why the \"[ANOMALOUS log]\" is an anomaly.\n",
        "To do this, first observe the pattern in the \"[NORMAL logs]\" which represent typical behavior.\n",
        "Then, contrast the \"[ANOMALOUS log]\" with this typical behavior, highlighting the key difference that makes it an anomaly.\n",
        "Provide your analysis under \"[ANALYSIS]\". Be specific and concise.\n",
        "\n",
        "### EXAMPLE OF GOOD ANALYSIS ###\n",
        "[NORMAL logs]\n",
        "2023-11-15 10:00:05 INFO: User 'alice' logged in successfully.\n",
        "2023-11-15 10:01:12 INFO: User 'bob' accessed the dashboard.\n",
        "2023-11-15 10:02:00 INFO: User 'charlie' updated their profile.\n",
        "\n",
        "[ANOMALOUS log]\n",
        "2023-11-15 10:03:45 CRITICAL: Database connection failed: timeout expired.\n",
        "\n",
        "[ANALYSIS]\n",
        "The normal logs show routine user activities such as logins and profile updates, all marked with INFO severity.\n",
        "The anomalous log, however, reports a CRITICAL system-level failure: a database connection timeout.\n",
        "This is an anomaly because it indicates a severe service disruption, differing in both severity (CRITICAL vs. INFO) and event type (system failure vs. user action) from the typical logs.\n",
        "\n",
        "### YOUR TASK ###\n",
        "[NORMAL logs]\n",
        "{normal_logs_text}\n",
        "\n",
        "[ANOMALOUS log]\n",
        "{anomaly_log_text}\n",
        "\n",
        "[ANALYSIS]\n",
        "\"\"\"\n",
        "        final_prompt = prompt_template.format(normal_logs_text=normal_logs_text, anomaly_log_text=anomaly_log)\n",
        "\n",
        "        try:\n",
        "            outputs = self.generator(final_prompt, max_new_tokens=max_new_tokens)\n",
        "            return textwrap.fill(outputs[0]['generated_text'], width=100)\n",
        "        except Exception as e:\n",
        "            return f\"Error during explanation generation: {e}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuSPMmmyJbE0",
        "outputId": "511f1e00-954f-4c24-b4b0-3a7acb4664d3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ladel/explanation.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Create the Main Pipeline Script\n",
        "%%writefile ladel/main.py\n",
        "import argparse\n",
        "import yaml\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
        "\n",
        "# Import our custom modules\n",
        "from ladel.data_loader import load_log_file\n",
        "from ladel.embedding import LogEmbedder\n",
        "from ladel.detection import get_anomaly_detector, train_and_predict\n",
        "from ladel.explanation import LogExplainer\n",
        "\n",
        "def run_pipeline(config_path: str, log_file_path: str):\n",
        "    \"\"\"Executes the full log anomaly detection and explanation pipeline.\"\"\"\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    print(\"--- Configuration Loaded ---\")\n",
        "    print(f\"Detection Model: {config['detection_model']}\")\n",
        "\n",
        "    log_lines = load_log_file(log_file_path)\n",
        "    if not log_lines:\n",
        "        print(\"No log lines to process. Exiting.\")\n",
        "        return\n",
        "\n",
        "    embedder = LogEmbedder(model_name=config['embedding_model'])\n",
        "    log_embeddings = embedder.encode(log_lines)\n",
        "\n",
        "    detector_name = config['detection_model']\n",
        "    detector_params = config[detector_name]\n",
        "    detector = get_anomaly_detector(detector_name, detector_params)\n",
        "    predictions, anomalous_indices = train_and_predict(detector, log_embeddings)\n",
        "\n",
        "    if predictions is None:\n",
        "        print(\"Anomaly detection failed. Cannot proceed.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"=\"*25 + \" RESULTS \" + \"=\"*25)\n",
        "    normal_indices = np.where(predictions == 1)[0]\n",
        "    print(f\"Total lines processed: {len(log_lines)}\")\n",
        "    print(f\"Normal lines found: {len(normal_indices)}\")\n",
        "    print(f\"Potential anomalies detected: {len(anomalous_indices)}\")\n",
        "\n",
        "    if len(anomalous_indices) == 0:\n",
        "        print(\"\\nNo anomalies found. All logs appear to be normal.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- Anomalous Logs ---\")\n",
        "    for idx in anomalous_indices:\n",
        "        print(f\"[ANOMALY] Line {idx+1}: {log_lines[idx]}\")\n",
        "\n",
        "    if config.get('explanation_enabled', False):\n",
        "        explainer = LogExplainer(model_name=config['llm_model'])\n",
        "        if explainer.generator:\n",
        "            print(\"\\n\" + \"=\"*20 + \" LLM EXPLANATIONS \" + \"=\"*20)\n",
        "            sample_size = min(config['num_normal_samples_for_context'], len(normal_indices))\n",
        "            if sample_size > 0:\n",
        "                sample_normal_indices = random.sample(list(normal_indices), sample_size)\n",
        "                normal_log_samples = [log_lines[i] for i in sample_normal_indices]\n",
        "                for i, anomaly_idx in enumerate(anomalous_indices):\n",
        "                    anomaly_log = log_lines[anomaly_idx]\n",
        "                    print(f\"\\n--- Analysis for Anomaly #{i+1} (Line {anomaly_idx+1}) ---\")\n",
        "                    print(f\"Anomalous Log: `{anomaly_log}`\")\n",
        "                    explanation = explainer.generate_explanation(anomaly_log, normal_log_samples)\n",
        "                    print(\"\\n[EXPLANATION]\")\n",
        "                    print(explanation)\n",
        "            else:\n",
        "                print(\"\\nCannot generate LLM explanations: No 'normal' logs found for baseline.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description=\"Log Anomaly Detection Pipeline\")\n",
        "    parser.add_argument(\"--config\", type=str, default=\"config.yaml\", help=\"Path to config file.\")\n",
        "    parser.add_argument(\"--log_file\", type=str, default=\"app.log\", help=\"Path to log file.\")\n",
        "    args = parser.parse_args()\n",
        "    run_pipeline(config_path=args.config, log_file_path=args.log_file)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPr6PSJ1JfUS",
        "outputId": "d3fa6d5e-a253-4311-dba3-c81db4e94eae"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ladel/main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Create Sample Log File for Testing\n",
        "%%writefile app.log\n",
        "2023-10-26 10:00:01 INFO: Application startup initiated. Version 1.2.3\n",
        "2023-10-26 10:00:02 INFO: Configuration loaded from /etc/app/config.xml\n",
        "2023-10-26 10:00:03 DEBUG: Database connection pool initialized with 10 connections.\n",
        "2023-10-26 10:00:04 INFO: Listening on port 8080\n",
        "2023-10-26 10:01:15 INFO: User 'alice' logged in successfully from IP 192.168.1.10\n",
        "2023-10-26 10:01:17 DEBUG: User 'alice' session created: session_abc123\n",
        "2023-10-26 10:01:20 INFO: Request received: GET /api/data?id=123 user='alice'\n",
        "2023-10-26 10:01:21 DEBUG: Querying database for item id=123\n",
        "2023-10-26 10:01:22 INFO: Data retrieval successful for item id=123. Rows: 1\n",
        "2023-10-26 10:01:25 INFO: User 'bob' logged in successfully from IP 192.168.1.12\n",
        "2023-10-26 10:01:27 DEBUG: User 'bob' session created: session_def456\n",
        "2023-10-26 10:02:00 INFO: Request received: POST /api/update user='alice'\n",
        "2023-10-26 10:02:01 DEBUG: Validating input data for update.\n",
        "2023-10-26 10:02:02 INFO: Data update successful for user 'alice'.\n",
        "2023-10-26 10:02:30 WARN: API rate limit approaching for user 'bob'. 950/1000 requests.\n",
        "2023-10-26 10:03:00 INFO: Scheduled job 'daily_backup' started.\n",
        "2023-10-26 10:03:05 INFO: Request received: GET /api/data?id=456 user='bob'\n",
        "2023-10-26 10:03:06 DEBUG: Querying database for item id=456\n",
        "2023-10-26 10:03:07 INFO: Data retrieval successful for item id=456. Rows: 1\n",
        "2023-10-26 10:03:10 ERROR: Failed to process payment for order_789. Reason: Insufficient funds. Customer: CUST007\n",
        "2023-10-26 10:03:12 DEBUG: Payment failure logged for order_789\n",
        "2023-10-26 10:04:00 INFO: User 'charlie' logged in successfully from IP 192.168.1.15\n",
        "2023-10-26 10:04:02 DEBUG: User 'charlie' session created: session_ghi789\n",
        "2023-10-26 10:04:05 INFO: Request received: GET /api/data?id=789 user='charlie'\n",
        "2023-10-26 10:04:06 DEBUG: Querying database for item id=789\n",
        "2023-10-26 10:04:07 INFO: Data retrieval successful for item id=789. Rows: 1\n",
        "2023-10-26 10:05:00 INFO: User 'alice' logged out. Session: session_abc123\n",
        "2023-10-26 10:05:15 WARN: High memory usage detected: 85% used.\n",
        "2023-10-26 10:05:18 INFO: Request received: DELETE /api/resource/xyz user='admin_priv'\n",
        "2023-10-26 10:05:19 INFO: Resource 'xyz' deleted successfully by 'admin_priv'.\n",
        "2023-10-26 10:06:00 INFO: User 'dave' attempted login from IP 203.0.113.45 - FAILED (Invalid Credentials)\n",
        "2023-10-26 10:06:01 INFO: User 'dave' attempted login from IP 203.0.113.45 - FAILED (Invalid Credentials)\n",
        "2023-10-26 10:06:02 INFO: User 'dave' attempted login from IP 203.0.113.45 - FAILED (Invalid Credentials)\n",
        "2023-10-26 10:06:03 INFO: User 'dave' attempted login from IP 203.0.113.45 - FAILED (Invalid Credentials)\n",
        "2023-10-26 10:06:04 WARN: Multiple failed login attempts for user 'dave' from IP 203.0.113.45. Account locked temporarily.\n",
        "2023-10-26 10:07:00 INFO: Request received: GET /api/system_health user='monitor_agent'\n",
        "2023-10-26 10:07:01 INFO: System health check: OK. CPU: 30%, Mem: 60%, Disk: 40%\n",
        "2023-10-26 10:08:00 INFO: Request received: GET /api/data?id=000 user='bob'\n",
        "2023-10-26 10:08:01 DEBUG: Querying database for item id=000\n",
        "2023-10-26 10:08:02 INFO: Data retrieval successful for item id=000. Rows: 1\n",
        "2023-10-26 10:09:00 INFO: Scheduled job 'data_cleanup' started.\n",
        "2023-10-26 10:10:00 INFO: User 'eve_hacker' logged in successfully from IP 10.0.0.5 (Internal Test Account)\n",
        "2023-10-26 10:10:05 INFO: Request received: GET /admin/config_dump user='eve_hacker'\n",
        "2023-10-26 10:10:06 CRITICAL: Unauthorized access attempt to /admin/config_dump by user 'eve_hacker' from IP 10.0.0.5.\n",
        "2023-10-26 10:10:07 INFO: User 'eve_hacker' session terminated. IP 10.0.0.5 blocked.\n",
        "2023-10-26 10:11:00 INFO: Data processing batch 'batch_alpha' started. Records: 10000\n",
        "2023-10-26 10:11:05 DEBUG: Processing record 1 of 10000 in 'batch_alpha'\n",
        "2023-10-26 10:11:10 DEBUG: Processing record 500 of 10000 in 'batch_alpha'\n",
        "2023-10-26 10:11:15 ERROR: Timeout while connecting to external service 'payment_gateway_v2'. URL: https://api.payments.example.com/charge\n",
        "2023-10-26 10:11:17 WARN: Retrying connection to 'payment_gateway_v2' (Attempt 1/3)\n",
        "2023-10-26 10:11:20 ERROR: Timeout while connecting to external service 'payment_gateway_v2'. URL: https://api.payments.example.com/charge\n",
        "2023-10-26 10:11:22 WARN: Retrying connection to 'payment_gateway_v2' (Attempt 2/3)\n",
        "2023-10-26 10:11:25 ERROR: Timeout while connecting to external service 'payment_gateway_v2'. URL: https://api.payments.example.com/charge\n",
        "2023-10-26 10:11:27 FATAL: Failed to connect to 'payment_gateway_v2' after 3 retries. Aborting batch 'batch_alpha'.\n",
        "2023-10-26 10:11:30 INFO: Data processing batch 'batch_alpha' failed.\n",
        "2023-10-26 10:12:00 INFO: User 'frank' logged in successfully from IP 192.168.2.22\n",
        "2023-10-26 10:12:05 INFO: Request to deprecated endpoint /api/v1/status by user 'frank'.\n",
        "2023-10-26 10:13:00 INFO: New feature flag 'beta_feature_X' enabled for user 'alice'.\n",
        "2023-10-26 10:14:00 INFO: Database schema migration version 3.4.1 started.\n",
        "2023-10-26 10:14:30 INFO: Database schema migration version 3.4.1 completed successfully.\n",
        "2023-10-26 10:15:00 INFO: System maintenance window starting in 60 minutes.\n",
        "2023-10-26 10:15:01 INFO: User 'grace' logged in successfully from IP 192.168.3.33\n",
        "2023-10-26 10:15:05 DEBUG: User 'grace' accessing /dashboard\n",
        "2023-10-26 10:15:10 INFO: Report 'monthly_sales' generated. Size: 2.5MB\n",
        "2023-10-26 10:16:00 INFO: Unexpected input format for field 'user_preference'. Value: '{\"theme\": \"dark mode\"}' instead of 'dark'. User: 'bob'\n",
        "2023-10-26 10:16:05 DEBUG: Attempting to parse 'user_preference' with fallback.\n",
        "2023-10-26 10:17:00 INFO: Email sent to admin@example.com: System CPU usage high\n",
        "2023-10-26 10:18:00 INFO: User 'heidi' logged in successfully from IP 192.168.1.18\n",
        "2023-10-26 10:18:05 DEBUG: Processing background task: image_resize_job_999\n",
        "2023-10-26 10:18:10 INFO: Background task 'image_resize_job_999' completed. Output: /path/to/resized_img.jpg\n",
        "2023-10-26 10:19:00 SEVERE: Core component 'MessageQueue' unresponsive. All queue operations paused.\n",
        "2023-10-26 10:19:05 INFO: Attempting to restart 'MessageQueue' component.\n",
        "2023-10-26 10:19:10 INFO: 'MessageQueue' component restarted successfully.\n",
        "2023-10-26 10:20:00 INFO: User 'ivan' logged in successfully from IP 172.16.0.5\n",
        "2023-10-26 10:20:05 INFO: File /tmp/big_upload.dat received. Size: 1024MB. Processing...\n",
        "2023-10-26 10:21:00 WARN: Disk space on /tmp is now 95% full. Cleaning up old files.\n",
        "2023-10-26 10:21:05 INFO: Old files in /tmp cleaned. Disk space at 70%.\n",
        "2023-10-26 10:22:00 INFO: User 'judy' logged in successfully from IP 192.168.4.50\n",
        "2023-10-26 10:22:05 DEBUG: User 'judy' initiated data export. Format: CSV\n",
        "2023-10-26 10:22:10 INFO: Data export for 'judy' (all_transactions.csv) completed.\n",
        "2023-10-26 10:23:00 INFO: Security audit: User 'admin_root' accessed sensitive data table 'user_credentials'. Justification: Scheduled audit.\n",
        "2023-10-26 10:24:00 INFO: Application performing self-test. All systems nominal.\n",
        "2023-10-26 10:25:00 INFO: User 'bob' updated profile. Changed email.\n",
        "2023-10-26 10:25:05 DEBUG: Sending email confirmation to bob_new_email@example.com\n",
        "2023-10-26 10:26:00 INFO: External API call to 'weather_service' successful. Temp: 15C\n",
        "2023-10-26 10:27:00 ERROR: Unhandled Python exception: KeyError 'missing_field' in module 'data_processor.py' line 245\n",
        "2023-10-26 10:27:01 DEBUG: Stack trace for KeyError: ... (omitted for brevity) ...\n",
        "2023-10-26 10:28:00 INFO: User 'ken' logged in successfully from IP 192.168.1.30\n",
        "2023-10-26 10:28:05 INFO: A peculiar cosmic ray event was detected by the server's internal chronometer. Time may be temporarily distorted.\n",
        "2023-10-26 10:29:00 INFO: Service 'recommendation_engine' reloaded with new model version 2.5.\n",
        "2023-10-26 10:30:00 INFO: All services healthy. Application running normally.\n",
        "2023-10-26 10:30:01 INFO: User 'laura' logged in successfully from IP 192.168.5.60\n",
        "2023-10-26 10:30:05 DEBUG: User 'laura' viewing product page 'prod_BXT7'\n",
        "2023-10-26 10:30:10 INFO: Added item 'prod_BXT7' to cart for user 'laura'\n",
        "2023-10-26 10:31:00 WARN: Certificate for 'external.partner.api.com' expiring in 7 days.\n",
        "2023-10-26 10:32:00 INFO: User 'mike' logged in successfully from IP 10.10.10.10\n",
        "2023-10-26 10:32:05 DEBUG: User 'mike' searching for 'rare_item_name'\n",
        "2023-10-26 10:32:10 INFO: Search for 'rare_item_name' yielded 0 results.\n",
        "2023-10-26 10:33:00 INFO: Resource allocation for 'batch_beta' increased. CPU: 4, Mem: 16GB\n",
        "2023-10-26 10:33:05 INFO: Data processing batch 'batch_beta' started. Records: 500\n",
        "2023-10-26 10:33:10 DEBUG: Processing record 1 of 500 in 'batch_beta'\n",
        "2023-10-26 10:34:00 INFO: New user registered: 'user_newbie_001'. Welcome email sent.\n",
        "2023-10-26 10:35:00 INFO: Database connection pool health: 8/10 connections active.\n",
        "2023-10-26 10:36:00 INFO: User 'bob' logged out. Session: session_def456\n",
        "2023-10-26 10:37:00 INFO: Request received: GET /api/data?id=XYZ user='charlie'\n",
        "2023-10-26 10:37:01 DEBUG: Querying database for item id=XYZ\n",
        "2023-10-26 10:37:02 INFO: Data retrieval successful for item id=XYZ. Rows: 1\n",
        "2023-10-26 10:38:00 INFO: System reboot scheduled for 2023-10-27 02:00:00 UTC.\n",
        "2023-10-26 10:39:00 INFO: User 'admin_ops' initiated manual cache flush.\n",
        "2023-10-26 10:39:05 INFO: Cache flush completed.\n",
        "2023-10-26 10:40:00 INFO: All good, nothing to report, just chilling.\n",
        "2023-10-26 10:40:01 INFO: Everything is perfectly fine and normal. Continue operations.\n",
        "2023-10-26 10:40:02 INFO: This is a standard operational message, number 345.\n",
        "2023-10-26 10:40:03 INFO: Processing complete for user 'zeta', all systems green.\n",
        "2023-10-26 10:40:04 INFO: Final log entry for this batch, signing off.\n",
        "2023-10-26 10:41:00 ERROR: Corrupted data packet received from sensor SENSOR_003. Data: 0xDEADBEEF...\n",
        "2023-10-26 10:42:00 INFO: Service 'user_auth' restarting due to minor glitch.\n",
        "2023-10-26 10:42:05 INFO: Service 'user_auth' back online.\n",
        "2023-10-26 10:43:00 WARN: API version v1 for /api/data is deprecated. Advise clients to use v2.\n",
        "2023-10-26 10:44:00 INFO: Performing routine data integrity check...\n",
        "2023-10-26 10:44:30 INFO: Data integrity check passed. No issues found.\n",
        "2023-10-26 10:45:00 INFO: Application shutting down gracefully. Goodbye!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WujNWgreMVGw",
        "outputId": "93d396eb-3c59-4e16-800d-142c7e31c351"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Run the Analysis Pipeline\n",
        "print(\"EXECUTING LOG ANALYSIS PIPELINE\")\n",
        "# We run the main script. It will automatically use 'config.yaml' and 'app.log'.\n",
        "!python ladel/main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syZLvxPbJuvW",
        "outputId": "ace86552-64cb-4430-edcf-3ee1a7e1d4cc"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXECUTING LOG ANALYSIS PIPELINE\n",
            "2025-06-10 14:25:39.347819: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1749565539.382524   19491 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1749565539.392688   19491 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-10 14:25:39.423989: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "--- Configuration Loaded ---\n",
            "Detection Model: isolation_forest\n",
            "Reading log file from: app.log\n",
            "Read 122 non-empty log lines.\n",
            "\n",
            "Loading sentence transformer model: 'all-MiniLM-L6-v2'...\n",
            "Model loaded successfully.\n",
            "Generating embeddings for log lines...\n",
            "Batches: 100% 4/4 [00:00<00:00, 14.14it/s]\n",
            "Generated 122 embeddings with dimension 384.\n",
            "\n",
            "Initializing detection model: isolation_forest\n",
            "Training IsolationForest for anomaly detection...\n",
            "Model identified 13 potential anomalies.\n",
            "\n",
            "========================= RESULTS =========================\n",
            "Total lines processed: 122\n",
            "Normal lines found: 109\n",
            "Potential anomalies detected: 13\n",
            "\n",
            "--- Anomalous Logs ---\n",
            "[ANOMALY] Line 4: 2023-10-26 10:00:04 INFO: Listening on port 8080\n",
            "[ANOMALY] Line 20: 2023-10-26 10:03:10 ERROR: Failed to process payment for order_789. Reason: Insufficient funds. Customer: CUST007\n",
            "[ANOMALY] Line 30: 2023-10-26 10:05:19 INFO: Resource 'xyz' deleted successfully by 'admin_priv'.\n",
            "[ANOMALY] Line 44: 2023-10-26 10:10:06 CRITICAL: Unauthorized access attempt to /admin/config_dump by user 'eve_hacker' from IP 10.0.0.5.\n",
            "[ANOMALY] Line 54: 2023-10-26 10:11:27 FATAL: Failed to connect to 'payment_gateway_v2' after 3 retries. Aborting batch 'batch_alpha'.\n",
            "[ANOMALY] Line 60: 2023-10-26 10:14:30 INFO: Database schema migration version 3.4.1 completed successfully.\n",
            "[ANOMALY] Line 65: 2023-10-26 10:16:00 INFO: Unexpected input format for field 'user_preference'. Value: '{\"theme\": \"dark mode\"}' instead of 'dark'. User: 'bob'\n",
            "[ANOMALY] Line 70: 2023-10-26 10:18:10 INFO: Background task 'image_resize_job_999' completed. Output: /path/to/resized_img.jpg\n",
            "[ANOMALY] Line 73: 2023-10-26 10:19:10 INFO: 'MessageQueue' component restarted successfully.\n",
            "[ANOMALY] Line 80: 2023-10-26 10:22:10 INFO: Data export for 'judy' (all_transactions.csv) completed.\n",
            "[ANOMALY] Line 86: 2023-10-26 10:27:00 ERROR: Unhandled Python exception: KeyError 'missing_field' in module 'data_processor.py' line 245\n",
            "[ANOMALY] Line 95: 2023-10-26 10:31:00 WARN: Certificate for 'external.partner.api.com' expiring in 7 days.\n",
            "[ANOMALY] Line 98: 2023-10-26 10:32:10 INFO: Search for 'rare_item_name' yielded 0 results.\n",
            "\n",
            "Setting up Hugging Face text-generation pipeline...\n",
            "GPU found. Loading model 'google/flan-t5-base' to GPU.\n",
            "LLM explainer loaded successfully!\n",
            "\n",
            "==================== LLM EXPLANATIONS ====================\n",
            "\n",
            "--- Analysis for Anomaly #1 (Line 4) ---\n",
            "Anomalous Log: `2023-10-26 10:00:04 INFO: Listening on port 8080`\n",
            "\n",
            "[EXPLANATION]\n",
            "The normal logs show routine user activities such as logins and profile updates, all marked with\n",
            "INFO severity. The anomalous log, however, reports a CRITICAL system-level failure: a database\n",
            "connection timeout. This is an anomaly because it indicates a severe service disruption, differing\n",
            "in both severity (CRITICAL vs. INFO) and event type (system failure vs. user action) from the\n",
            "typical logs.\n",
            "\n",
            "--- Analysis for Anomaly #2 (Line 20) ---\n",
            "Anomalous Log: `2023-10-26 10:03:10 ERROR: Failed to process payment for order_789. Reason: Insufficient funds. Customer: CUST007`\n",
            "\n",
            "[EXPLANATION]\n",
            "[ANOMALOUS log] 2023-10-26 10:41:00 INFO: User 'alice' logged in successfully. 2023-10-26 10:01:12\n",
            "INFO: User 'bob' accessed the dashboard. 2023-10-26 10:03:45 INFO: Database connection failed:\n",
            "timeout expired. 2023-10-26 10:35:00 INFO: New user registered: 'user_newbie_001'. Welcome email\n",
            "sent. 2023-10-26 10:21:05 INFO: Old files in /tmp cleaned. Disk\n",
            "\n",
            "--- Analysis for Anomaly #3 (Line 30) ---\n",
            "Anomalous Log: `2023-10-26 10:05:19 INFO: Resource 'xyz' deleted successfully by 'admin_priv'.`\n",
            "\n",
            "[EXPLANATION]\n",
            "The normal logs show routine user activities such as logins and profile updates, all marked with\n",
            "INFO severity. The anomalous log, however, reports a CRITICAL system-level failure: a database\n",
            "connection timeout. This is an anomaly because it indicates a severe service disruption, differing\n",
            "in both severity (CRITICAL vs. INFO) and event type (system failure vs. user action) from the\n",
            "typical logs.\n",
            "\n",
            "--- Analysis for Anomaly #4 (Line 44) ---\n",
            "Anomalous Log: `2023-10-26 10:10:06 CRITICAL: Unauthorized access attempt to /admin/config_dump by user 'eve_hacker' from IP 10.0.0.5.`\n",
            "\n",
            "[EXPLANATION]\n",
            "[ANOMALOUS log] 2023-10-26 10:41:00 INFO: User 'alice' logged in successfully. 2023-10-26 10:01:12\n",
            "INFO: User 'bob' accessed the dashboard. 2023-10-26 10:35:00 INFO: Database connection pool health:\n",
            "8/10 connections active. 2023-10-26 10:35:00 INFO: New user registered: 'user_newbie_001'. Welcome\n",
            "email sent. 2023-10-26 10:21:05 INFO: Old files in /tmp cleaned. Dis\n",
            "\n",
            "--- Analysis for Anomaly #5 (Line 54) ---\n",
            "Anomalous Log: `2023-10-26 10:11:27 FATAL: Failed to connect to 'payment_gateway_v2' after 3 retries. Aborting batch 'batch_alpha'.`\n",
            "\n",
            "[EXPLANATION]\n",
            "The normal logs show routine user activities such as logins and profile updates, all marked with\n",
            "INFO severity. The anomalous log, however, reports a CRITICAL system-level failure: a database\n",
            "connection timeout. This is an anomaly because it indicates a severe service disruption, differing\n",
            "in both severity (CRITICAL vs. INFO) and event type (system failure vs. user action) from the\n",
            "typical logs.\n",
            "\n",
            "--- Analysis for Anomaly #6 (Line 60) ---\n",
            "Anomalous Log: `2023-10-26 10:14:30 INFO: Database schema migration version 3.4.1 completed successfully.`\n",
            "\n",
            "[EXPLANATION]\n",
            "The normal logs show routine user activities such as logins and profile updates, all marked with\n",
            "INFO severity. The anomalous log, however, reports a CRITICAL system-level failure: a database\n",
            "connection timeout. This is an anomaly because it indicates a severe service disruption, differing\n",
            "in both severity (CRITICAL vs. INFO) and event type (system failure vs. user action) from the\n",
            "typical logs.\n",
            "\n",
            "--- Analysis for Anomaly #7 (Line 65) ---\n",
            "Anomalous Log: `2023-10-26 10:16:00 INFO: Unexpected input format for field 'user_preference'. Value: '{\"theme\": \"dark mode\"}' instead of 'dark'. User: 'bob'`\n",
            "\n",
            "[EXPLANATION]\n",
            "The normal logs show routine user activities such as logins and profile updates, all marked with\n",
            "INFO severity. The anomalous log, however, reports a CRITICAL system-level failure: a database\n",
            "connection timeout. This is an anomaly because it indicates a severe service disruption, differing\n",
            "in both severity (CRITICAL vs. INFO) and event type (system failure vs. user action) from the\n",
            "typical logs.\n",
            "\n",
            "--- Analysis for Anomaly #8 (Line 70) ---\n",
            "Anomalous Log: `2023-10-26 10:18:10 INFO: Background task 'image_resize_job_999' completed. Output: /path/to/resized_img.jpg`\n",
            "\n",
            "[EXPLANATION]\n",
            "The normal logs show routine user activities such as logins and profile updates, all marked with\n",
            "INFO severity. The anomalous log, however, reports a CRITICAL system-level failure: a database\n",
            "connection timeout. This is an anomaly because it indicates a severe service disruption, differing\n",
            "in both severity (CRITICAL vs. INFO) and event type (system failure vs. user action) from the\n",
            "typical logs.\n",
            "\n",
            "--- Analysis for Anomaly #9 (Line 73) ---\n",
            "Anomalous Log: `2023-10-26 10:19:10 INFO: 'MessageQueue' component restarted successfully.`\n",
            "\n",
            "[EXPLANATION]\n",
            "The normal logs show routine user activities such as logins and profile updates, all marked with\n",
            "INFO severity. The anomalous log, however, reports a CRITICAL system-level failure: a database\n",
            "connection timeout. This is an anomaly because it indicates a severe service disruption, differing\n",
            "in both severity (CRITICAL vs. INFO) and event type (system failure vs. user action) from the\n",
            "typical logs.\n",
            "\n",
            "--- Analysis for Anomaly #10 (Line 80) ---\n",
            "Anomalous Log: `2023-10-26 10:22:10 INFO: Data export for 'judy' (all_transactions.csv) completed.`\n",
            "\n",
            "[EXPLANATION]\n",
            "The normal logs show routine user activities such as logins and profile updates, all marked with\n",
            "INFO severity. The anomalous log, however, reports a CRITICAL system-level failure: a database\n",
            "connection timeout. This is an anomaly because it indicates a severe service disruption, differing\n",
            "in both severity (CRITICAL vs. INFO) and event type (system failure vs. user action) from the\n",
            "typical logs.\n",
            "\n",
            "--- Analysis for Anomaly #11 (Line 86) ---\n",
            "Anomalous Log: `2023-10-26 10:27:00 ERROR: Unhandled Python exception: KeyError 'missing_field' in module 'data_processor.py' line 245`\n",
            "\n",
            "[EXPLANATION]\n",
            "[ANOMALOUS log] 2023-10-26 10:41:00 INFO: User 'alice' logged in successfully. 2023-10-26 10:01:12\n",
            "INFO: User 'bob' accessed the dashboard. 2023-10-26 10:35:00 INFO: Database connection pool health:\n",
            "8/10 connections active. 2023-10-26 10:11:25 INFO: New user registered: 'user_newbie_001'. Welcome\n",
            "email sent. 2023-10-26 10:21:05 INFO: Old files in /tmp cleaned.\n",
            "\n",
            "--- Analysis for Anomaly #12 (Line 95) ---\n",
            "Anomalous Log: `2023-10-26 10:31:00 WARN: Certificate for 'external.partner.api.com' expiring in 7 days.`\n",
            "\n",
            "[EXPLANATION]\n",
            "The normal logs show routine user activities such as logins and profile updates, all marked with\n",
            "INFO severity. The anomalous log, however, reports a CRITICAL system-level failure: a database\n",
            "connection timeout. This is an anomaly because it indicates a severe service disruption, differing\n",
            "in both severity (CRITICAL vs. INFO) and event type (system failure vs. user action) from the\n",
            "typical logs.\n",
            "\n",
            "--- Analysis for Anomaly #13 (Line 98) ---\n",
            "Anomalous Log: `2023-10-26 10:32:10 INFO: Search for 'rare_item_name' yielded 0 results.`\n",
            "\n",
            "[EXPLANATION]\n",
            "The normal logs show routine user activities such as logins and profile updates, all marked with\n",
            "INFO severity. The anomalous log, however, reports a CRITICAL system-level failure: a database\n",
            "connection timeout. This is an anomaly because it indicates a severe service disruption, differing\n",
            "in both severity (CRITICAL vs. INFO) and event type (system failure vs. user action) from the\n",
            "typical logs.\n"
          ]
        }
      ]
    }
  ]
}